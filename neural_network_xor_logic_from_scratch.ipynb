{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Hidden Layer Neural Network XOR Logic Code from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First I import numpy library and matplotlib used to display loss curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then I defined the inputs and structure of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are XOR inputs\n",
    "x=np.array([[0,0,1,1],[0,1,0,1]])\n",
    "# These are XOR outputs\n",
    "y=np.array([[0,1,1,0]])\n",
    "# Number of inputs\n",
    "n_x = 2\n",
    "# Number of neurons in output layer\n",
    "n_y = 1\n",
    "# Number of neurons in hidden layer\n",
    "n_h = 2\n",
    "# Total training examples\n",
    "m = x.shape[1]\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "# Define random seed for consistent results\n",
    "np.random.seed(2)\n",
    "# Define weight matrices for neural network\n",
    "w1 = np.random.rand(n_h,n_x)   # Weight matrix for hidden layer\n",
    "w2 = np.random.rand(n_y,n_h)   # Weight matrix for output layer\n",
    "# I didnt use bias units\n",
    "# We will use this list to accumulate losses\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I define the important processes as Python methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used sigmoid activation function for hidden layer and output\n",
    "def sigmoid(z):\n",
    "    z= 1/(1+np.exp(-z))\n",
    "    return z\n",
    "\n",
    "# Forward propagation\n",
    "def forward_prop(w1,w2,x):\n",
    "    z1 = np.dot(w1,x)\n",
    "    a1 = sigmoid(z1)    \n",
    "    z2 = np.dot(w2,a1)\n",
    "    a2 = sigmoid(z2)\n",
    "    return z1,a1,z2,a2\n",
    "\n",
    "# Backward propagation\n",
    "def back_prop(m,w1,w2,z1,a1,z2,a2,y):\n",
    "    \n",
    "    dz2 = a2-y\n",
    "    dw2 = np.dot(dz2,a1.T)/m\n",
    "    dz1 = np.dot(w2.T,dz2) * a1*(1-a1)\n",
    "    dw1 = np.dot(dz1,x.T)/m\n",
    "    dw1 = np.reshape(dw1,w1.shape)\n",
    "    \n",
    "    dw2 = np.reshape(dw2,w2.shape)    \n",
    "    return dz2,dw2,dz1,dw1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we run the neural network for 10000 iterations and observe the loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6945972  0.69298621]\n",
      " [4.95603514 4.90910154]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-8.76869413,  6.71602913]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations = 10000\n",
    "for i in range(iterations):\n",
    "    z1,a1,z2,a2 = forward_prop(w1,w2,x)\n",
    "    loss = -(1/m)*np.sum(y*np.log(a2)+(1-y)*np.log(1-a2))\n",
    "    losses.append(loss)\n",
    "    da2,dw2,dz1,dw1 = back_prop(m,w1,w2,z1,a1,z2,a2,y)\n",
    "    w2 = w2-lr*dw2\n",
    "    w1 = w1-lr*dw1\n",
    "\n",
    "# We plot losses to see how our network is doing\n",
    "# plt.plot(losses)\n",
    "# plt.xlabel(\"EPOCHS\")\n",
    "# plt.ylabel(\"Loss value\")\n",
    "# print(w1)\n",
    "print(w1)\n",
    "w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now after training we see how our neural network is doing in terms of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w1,w2,input):\n",
    "    z1,a1,z2,a2 = forward_prop(w1,w2,test)\n",
    "    a2 = np.squeeze(a2)\n",
    "    if a2>=0.5:\n",
    "        print(\"For input\", [i[0] for i in input], \"output is 1\")# ['{:.2f}'.format(i) for i in x])\n",
    "    else:\n",
    "        print(\"For input\", [i[0] for i in input], \"output is 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are the predictions of our trained neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input [np.int64(1), np.int64(0)] output is 1\n",
      "For input [np.int64(0), np.int64(0)] output is 0\n",
      "For input [np.int64(0), np.int64(1)] output is 1\n",
      "For input [np.int64(1), np.int64(1)] output is 0\n",
      "For input [np.int64(1), np.int64(1)] output is 0\n",
      "For input [np.int64(1), np.int64(1)] output is 0\n"
     ]
    }
   ],
   "source": [
    "test = np.array([[1],[0]])\n",
    "predict(w1,w2,test)\n",
    "test = np.array([[0],[0]])\n",
    "predict(w1,w2,test)\n",
    "test = np.array([[0],[1]])\n",
    "predict(w1,w2,test)\n",
    "test = np.array([[1],[1]])\n",
    "predict(w1,w2,test)\n",
    "test = np.array([[1],[1]])\n",
    "predict(w1,w2,test)\n",
    "test = np.array([[1],[1]])\n",
    "predict(w1,w2,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
